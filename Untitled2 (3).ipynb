{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Bidirectional, LSTM, Input\n",
        "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load and preprocess the data\n",
        "with open(\"que&ans.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    data = file.read().split('\\n')\n",
        "\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for line in data:\n",
        "    if line.startswith('|Q|'):\n",
        "        questions.append(line[3:])\n",
        "    elif line.startswith('|A|'):\n",
        "        answers.append(line[3:])\n",
        "\n",
        "# Tokenize the data using DistilBERT tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "questions_tokens = tokenizer(questions, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "input_ids = questions_tokens['input_ids']\n",
        "attention_mask = questions_tokens['attention_mask']\n",
        "\n",
        "# Create input sequences and pad them\n",
        "input_sequences = []\n",
        "for i in range(len(questions)):\n",
        "    for j in range(1, len(input_ids[i])):\n",
        "        n_gram_sequence = input_ids[i][:j + 1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_length = max(len(seq) for seq in input_sequences)\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = np.array(np.eye(np.max(input_sequences) + 1)[y])\n",
        "\n",
        "# Build the model with DistilBERT layers\n",
        "distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased', trainable=False)\n",
        "inputs = Input(shape=(max_sequence_length-1,), dtype=tf.int32)\n",
        "distilbert_output = distilbert_model(inputs)[0]\n",
        "bi_lstm = Bidirectional(LSTM(100))(distilbert_output)\n",
        "output = Dense(np.max(input_sequences) + 1, activation='softmax')(bi_lstm)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=20, verbose=1)\n",
        "\n",
        "# Save the tokenizer and model\n",
        "model.save('chatbot_model_distilbert.h5')\n",
        "with open('tokenizer_distilbert.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vBJoYRhAQD7",
        "outputId": "3f53bab0-d0ca-46d4-e3b1-0326b18f1b5b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "39/39 [==============================] - 12s 58ms/step - loss: 6.1819 - accuracy: 0.5053\n",
            "Epoch 2/20\n",
            "39/39 [==============================] - 2s 56ms/step - loss: 2.9118 - accuracy: 0.5216\n",
            "Epoch 3/20\n",
            "39/39 [==============================] - 2s 56ms/step - loss: 2.5153 - accuracy: 0.5216\n",
            "Epoch 4/20\n",
            "39/39 [==============================] - 2s 56ms/step - loss: 2.3629 - accuracy: 0.5396\n",
            "Epoch 5/20\n",
            "39/39 [==============================] - 2s 56ms/step - loss: 2.2798 - accuracy: 0.5657\n",
            "Epoch 6/20\n",
            "39/39 [==============================] - 2s 56ms/step - loss: 2.2172 - accuracy: 0.5682\n",
            "Epoch 7/20\n",
            "39/39 [==============================] - 2s 56ms/step - loss: 2.1523 - accuracy: 0.6000\n",
            "Epoch 8/20\n",
            "39/39 [==============================] - 2s 56ms/step - loss: 2.0984 - accuracy: 0.5984\n",
            "Epoch 9/20\n",
            "39/39 [==============================] - 2s 56ms/step - loss: 2.0471 - accuracy: 0.6122\n",
            "Epoch 10/20\n",
            "39/39 [==============================] - 2s 57ms/step - loss: 2.0007 - accuracy: 0.6245\n",
            "Epoch 11/20\n",
            "39/39 [==============================] - 2s 57ms/step - loss: 1.9734 - accuracy: 0.6212\n",
            "Epoch 12/20\n",
            "39/39 [==============================] - 2s 57ms/step - loss: 1.9388 - accuracy: 0.6237\n",
            "Epoch 13/20\n",
            "39/39 [==============================] - 2s 57ms/step - loss: 1.9108 - accuracy: 0.6302\n",
            "Epoch 14/20\n",
            "39/39 [==============================] - 2s 56ms/step - loss: 1.8860 - accuracy: 0.6294\n",
            "Epoch 15/20\n",
            "39/39 [==============================] - 2s 56ms/step - loss: 1.8603 - accuracy: 0.6310\n",
            "Epoch 16/20\n",
            "39/39 [==============================] - 2s 56ms/step - loss: 1.8381 - accuracy: 0.6327\n",
            "Epoch 17/20\n",
            "39/39 [==============================] - 2s 56ms/step - loss: 1.8113 - accuracy: 0.6302\n",
            "Epoch 18/20\n",
            "39/39 [==============================] - 2s 57ms/step - loss: 1.7873 - accuracy: 0.6351\n",
            "Epoch 19/20\n",
            "39/39 [==============================] - 2s 56ms/step - loss: 1.7658 - accuracy: 0.6384\n",
            "Epoch 20/20\n",
            "39/39 [==============================] - 2s 56ms/step - loss: 1.7386 - accuracy: 0.6359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the tokenizer and model with custom objects\n",
        "custom_objects = {'TFDistilBertModel': TFDistilBertModel}\n",
        "model = load_model('chatbot_model_distilbert.h5', custom_objects=custom_objects, compile=False)\n",
        "\n",
        "with open('tokenizer_distilbert.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "# Get the max sequence length used during training\n",
        "if model.layers:\n",
        "    max_sequence_length = model.layers[1].input_shape[1]\n",
        "\n",
        "# Function to generate a response\n",
        "def generate_response(question):\n",
        "    if not question:\n",
        "        return \"Chatbot: Please enter a question.\"\n",
        "\n",
        "    input_ids = tokenizer.encode(question, return_tensors='tf', max_length=max_sequence_length, padding='max_length', truncation=True)\n",
        "    response_ids = []\n",
        "\n",
        "    # for _ in range(max_sequence_length):\n",
        "    #     logits = model.predict(input_ids)\n",
        "    #     predicted_id = np.argmax(logits)\n",
        "    #     response_ids.append(predicted_id)\n",
        "\n",
        "    #     # Update input_ids for the next iteration\n",
        "    #     input_ids = np.concatenate([input_ids, np.array([[predicted_id]])], axis=1)\n",
        "\n",
        "    # # Ensure the response length does not exceed the max_sequence_length\n",
        "    # response_tokens = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "    for i in range(max_sequence_length - 1):\n",
        "         if i >= len(response_ids): break\n",
        "\n",
        "         logits = model.predict(input_ids)\n",
        "         predicted_id = np.argmax(logits)\n",
        "\n",
        "         if predicted_id == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "         response_ids.append(predicted_id)\n",
        "\n",
        "         input_ids = np.concatenate([input_ids,\n",
        "                              np.array([[predicted_id]])], axis=1)[:, -max_sequence_length:]\n",
        "\n",
        "    response_tokens = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "    return \"Chatbot: \" + response_tokens\n",
        "\n",
        "# Chat with the model\n",
        "print(\"Chatbot: Hi! I'm your chatbot. You can type 'exit' to end the conversation.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "    elif user_input.lower() == 'start':\n",
        "        print(\"Chatbot: You can start typing your questions.\")\n",
        "    else:\n",
        "        response = generate_response(user_input)\n",
        "        print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBSJxvnVICYV",
        "outputId": "da68673c-f3a0-41d1-97ef-9ec769393a83"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: Hi! I'm your chatbot. You can type 'exit' to end the conversation.\n",
            "You: what is the difference between policy and procedure?\n",
            "Chatbot: \n",
            "You: \n",
            "Chatbot: Please enter a question.\n",
            "You: hi\n",
            "Chatbot: \n",
            "You: exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    }
  ]
}