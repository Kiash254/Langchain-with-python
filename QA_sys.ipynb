{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Bidirectional, LSTM, Input\n",
        "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "kok-dlu_8o_F"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load and preprocess the data\n",
        "with open(\"que&ans.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    data = file.read().split('\\n')\n",
        "\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for line in data:\n",
        "    if line.startswith('|Q|'):\n",
        "        questions.append(line[3:])\n",
        "    elif line.startswith('|A|'):\n",
        "        answers.append(line[3:])\n"
      ],
      "metadata": {
        "id": "2DzZqU58_mRT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenize the data using DistilBERT tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "questions_tokens = tokenizer(questions, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "input_ids = questions_tokens['input_ids']\n",
        "attention_mask = questions_tokens['attention_mask']"
      ],
      "metadata": {
        "id": "eghEdIsf_sCZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input sequences and pad them\n",
        "input_sequences = []\n",
        "for i in range(len(questions)):\n",
        "    for j in range(1, len(input_ids[i])):\n",
        "        n_gram_sequence = input_ids[i][:j + 1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_length = max(len(seq) for seq in input_sequences)\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = np.array(np.eye(np.max(input_sequences) + 1)[y])"
      ],
      "metadata": {
        "id": "jE9EDi4l_4FT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Build the model with DistilBERT layers\n",
        "distilbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased', trainable=False)\n",
        "inputs = Input(shape=(max_sequence_length-1,), dtype=tf.int32)\n",
        "distilbert_output = distilbert_model(inputs)[0]\n",
        "bi_lstm = Bidirectional(LSTM(100))(distilbert_output)\n",
        "output = Dense(np.max(input_sequences) + 1, activation='softmax')(bi_lstm)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk-8gHUy_8X9",
        "outputId": "fa89f016-f2b4-4703-90c3-aa40a511ee27"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X, y, epochs=20, verbose=1)\n",
        "\n",
        "# Save the tokenizer and model\n",
        "model.save('chatbot_model_distilbert.h5')\n",
        "with open('tokenizer_distilbert.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vBJoYRhAQD7",
        "outputId": "1b49e402-d6c8-4fe4-bb44-e03daafc82c8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "70/70 [==============================] - 17s 79ms/step - loss: 4.2154 - accuracy: 0.6622\n",
            "Epoch 2/20\n",
            "70/70 [==============================] - 5s 75ms/step - loss: 1.8402 - accuracy: 0.6752\n",
            "Epoch 3/20\n",
            "70/70 [==============================] - 6s 79ms/step - loss: 1.6545 - accuracy: 0.6995\n",
            "Epoch 4/20\n",
            "70/70 [==============================] - 5s 76ms/step - loss: 1.5870 - accuracy: 0.7095\n",
            "Epoch 5/20\n",
            "70/70 [==============================] - 5s 78ms/step - loss: 1.5364 - accuracy: 0.7176\n",
            "Epoch 6/20\n",
            "70/70 [==============================] - 5s 79ms/step - loss: 1.4764 - accuracy: 0.7284\n",
            "Epoch 7/20\n",
            "70/70 [==============================] - 5s 76ms/step - loss: 1.4282 - accuracy: 0.7369\n",
            "Epoch 8/20\n",
            "70/70 [==============================] - 6s 80ms/step - loss: 1.3905 - accuracy: 0.7369\n",
            "Epoch 9/20\n",
            "70/70 [==============================] - 5s 75ms/step - loss: 1.3603 - accuracy: 0.7401\n",
            "Epoch 10/20\n",
            "70/70 [==============================] - 6s 79ms/step - loss: 1.3312 - accuracy: 0.7410\n",
            "Epoch 11/20\n",
            "70/70 [==============================] - 6s 85ms/step - loss: 1.3064 - accuracy: 0.7410\n",
            "Epoch 12/20\n",
            "70/70 [==============================] - 5s 75ms/step - loss: 1.2866 - accuracy: 0.7423\n",
            "Epoch 13/20\n",
            "70/70 [==============================] - 6s 79ms/step - loss: 1.2576 - accuracy: 0.7464\n",
            "Epoch 14/20\n",
            "70/70 [==============================] - 5s 76ms/step - loss: 1.2280 - accuracy: 0.7491\n",
            "Epoch 15/20\n",
            "70/70 [==============================] - 5s 76ms/step - loss: 1.2087 - accuracy: 0.7545\n",
            "Epoch 16/20\n",
            "70/70 [==============================] - 5s 79ms/step - loss: 1.1830 - accuracy: 0.7514\n",
            "Epoch 17/20\n",
            "70/70 [==============================] - 5s 76ms/step - loss: 1.1548 - accuracy: 0.7527\n",
            "Epoch 18/20\n",
            "70/70 [==============================] - 5s 78ms/step - loss: 1.1223 - accuracy: 0.7613\n",
            "Epoch 19/20\n",
            "70/70 [==============================] - 5s 76ms/step - loss: 1.1002 - accuracy: 0.7617\n",
            "Epoch 20/20\n",
            "70/70 [==============================] - 5s 75ms/step - loss: 1.0734 - accuracy: 0.7689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the tokenizer and model with custom objects\n",
        "custom_objects = {'TFDistilBertModel': TFDistilBertModel}\n",
        "model = load_model('chatbot_model_distilbert.h5', custom_objects=custom_objects, compile=False)\n",
        "\n",
        "with open('tokenizer_distilbert.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "# Get the max sequence length used during training\n",
        "if model.layers:\n",
        "    max_sequence_length = model.layers[1].input_shape[1]\n",
        "\n",
        "# Load question-answer pairs from text file into a dictionary\n",
        "qa_dict = {}\n",
        "with open('que&ans.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    for i in range(0, len(lines), 2):\n",
        "        question = lines[i].strip()[3:]  # Remove '|Q|'\n",
        "        answer = lines[i+1].strip()[3:]  # Remove '|A|'\n",
        "        qa_dict[question] = answer\n",
        "\n",
        "# Function to generate a response\n",
        "def generate_response(question):\n",
        "    # Check if the question is in the dictionary\n",
        "    if question in qa_dict:\n",
        "        return \"Chatbot: \" + qa_dict[question]\n",
        "\n",
        "    # If not, generate a response using the model\n",
        "    input_ids = tokenizer.encode(question, return_tensors='tf', max_length=max_sequence_length, padding='max_length', truncation=True)\n",
        "    response_ids = []\n",
        "\n",
        "    for i in range(max_sequence_length - 1):\n",
        "        if i >= len(response_ids): break\n",
        "\n",
        "        logits = model.predict(input_ids)\n",
        "        predicted_id = np.argmax(logits)\n",
        "\n",
        "        if predicted_id == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "        response_ids.append(predicted_id)\n",
        "\n",
        "        input_ids = np.concatenate([input_ids, np.array([[predicted_id]])], axis=1)[:, -max_sequence_length:]\n",
        "\n",
        "    response_tokens = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "\n",
        "    # If the model fails to generate a response, raise an exception\n",
        "    if not response_tokens:\n",
        "        raise Exception(\"Model failed to generate a response\")\n",
        "\n",
        "    return \"Chatbot: \" + response_tokens\n",
        "\n",
        "# Chat with the model\n",
        "print(\"Chatbot: Hi! I'm your chatbot. You can type 'exit' to end the conversation.\")\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "        else:\n",
        "            response = generate_response(user_input)\n",
        "            print(response)\n",
        "    except Exception as e:\n",
        "        print(\"Chatbot: I'm sorry, Please take the Question from the txt file. Goodbye!\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBSJxvnVICYV",
        "outputId": "194591d7-eded-4cef-bdff-050336c9f8e0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: Hi! I'm your chatbot. You can type 'exit' to end the conversation.\n",
            "You: Who can checkout a Spark laptop?\n",
            "Chatbot: Currently enrolled (in the active semester) UNT students, whom are in good standing with UNT and the Library are eligible to check out a laptop from the Spark\n",
            "You: What is the difference between policy and procedure?\n",
            "Chatbot: A quick way to distinguish a policy from a procedure is that a policy states “Why” the institution takes certain positions on an issue, and a procedure outlines “How” the institution will implement this policy position. For a more detailed break-down of the difference, please check out our Policies v. Procedures Comparison Table page.\n",
            "You: what is computer science ?\n",
            "Chatbot: I'm sorry, Please take the Question from the txt file. Goodbye!\n"
          ]
        }
      ]
    }
  ]
}